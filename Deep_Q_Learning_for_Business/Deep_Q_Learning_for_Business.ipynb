{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Q-Learning for Business.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMPZH272fCh7j7MgmU885YG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veerkalburgi/AI_projects/blob/master/Deep_Q_Learning_for_Business/Deep_Q_Learning_for_Business.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AmLRUzOsP9U",
        "colab_type": "text"
      },
      "source": [
        "# **Step1: Building the Environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoLNHunerwEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AI for Business - Minimize cost with Deep Q-Learning\n",
        "# Building the Environment\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "\n",
        "# BUILDING THE ENVIRONMENT IN A CLASS\n",
        "\n",
        "class Environment(object):\n",
        "    \n",
        "    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE ENVIRONMENT\n",
        "    \n",
        "    def __init__(self, optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 10, initial_rate_data = 60):\n",
        "        self.monthly_atmospheric_temperatures = [1.0, 5.0, 7.0, 10.0, 11.0, 20.0, 23.0, 24.0, 22.0, 10.0, 5.0, 1.0]\n",
        "        self.initial_month = initial_month\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[initial_month]\n",
        "        self.optimal_temperature = optimal_temperature\n",
        "        self.min_temperature = -20\n",
        "        self.max_temperature = 80\n",
        "        self.min_number_users = 10\n",
        "        self.max_number_users = 100\n",
        "        self.max_update_users = 5\n",
        "        self.min_rate_data = 20\n",
        "        self.max_rate_data = 300\n",
        "        self.max_update_data = 10\n",
        "        self.initial_number_users = initial_number_users\n",
        "        self.current_number_users = initial_number_users\n",
        "        self.initial_rate_data = initial_rate_data\n",
        "        self.current_rate_data = initial_rate_data\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        self.temperature_ai = self.intrinsic_temperature\n",
        "        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0\n",
        "        self.total_energy_ai = 0.0\n",
        "        self.total_energy_noai = 0.0\n",
        "        self.reward = 0.0\n",
        "        self.game_over = 0\n",
        "        self.train = 1\n",
        "\n",
        "    # MAKING A METHOD THAT UPDATES THE ENVIRONMENT RIGHT AFTER THE AI PLAYS AN ACTION\n",
        "    \n",
        "    def update_env(self, direction, energy_ai, month):\n",
        "        \n",
        "        # GETTING THE REWARD\n",
        "        \n",
        "        # Computing the energy spent by the server's cooling system when there is no AI\n",
        "        energy_noai = 0\n",
        "        if (self.temperature_noai < self.optimal_temperature[0]):\n",
        "            energy_noai = self.optimal_temperature[0] - self.temperature_noai\n",
        "            self.temperature_noai = self.optimal_temperature[0]\n",
        "        elif (self.temperature_noai > self.optimal_temperature[1]):\n",
        "            energy_noai = self.temperature_noai - self.optimal_temperature[1]\n",
        "            self.temperature_noai = self.optimal_temperature[1]\n",
        "        # Computing the Reward\n",
        "        self.reward = energy_noai - energy_ai\n",
        "        # Scaling the Reward\n",
        "        self.reward = 1e-3 * self.reward\n",
        "        \n",
        "        # GETTING THE NEXT STATE\n",
        "        \n",
        "        # Updating the atmospheric temperature\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[month]\n",
        "        # Updating the number of users\n",
        "        self.current_number_users += np.random.randint(-self.max_update_users, self.max_update_users)\n",
        "        if (self.current_number_users > self.max_number_users):\n",
        "            self.current_number_users = self.max_number_users\n",
        "        elif (self.current_number_users < self.min_number_users):\n",
        "            self.current_number_users = self.min_number_users\n",
        "        # Updating the rate of data\n",
        "        self.current_rate_data += np.random.randint(-self.max_update_data, self.max_update_data)\n",
        "        if (self.current_rate_data > self.max_rate_data):\n",
        "            self.current_rate_data = self.max_rate_data\n",
        "        elif (self.current_rate_data < self.min_rate_data):\n",
        "            self.current_rate_data = self.min_rate_data\n",
        "        # Computing the Delta of Intrinsic Temperature\n",
        "        past_intrinsic_temperature = self.intrinsic_temperature\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        delta_intrinsic_temperature = self.intrinsic_temperature - past_intrinsic_temperature\n",
        "        # Computing the Delta of Temperature caused by the AI\n",
        "        if (direction == -1):\n",
        "            delta_temperature_ai = -energy_ai\n",
        "        elif (direction == 1):\n",
        "            delta_temperature_ai = energy_ai\n",
        "        # Updating the new Server's Temperature when there is the AI\n",
        "        self.temperature_ai += delta_intrinsic_temperature + delta_temperature_ai\n",
        "        # Updating the new Server's Temperature when there is no AI\n",
        "        self.temperature_noai += delta_intrinsic_temperature\n",
        "        \n",
        "        # GETTING GAME OVER\n",
        "        \n",
        "        if (self.temperature_ai < self.min_temperature):\n",
        "            if (self.train == 1):\n",
        "                self.game_over = 1\n",
        "            else:\n",
        "                self.total_energy_ai += self.optimal_temperature[0] - self.temperature_ai\n",
        "                self.temperature_ai = self.optimal_temperature[0]\n",
        "        elif (self.temperature_ai > self.max_temperature):\n",
        "            if (self.train == 1):\n",
        "                self.game_over = 1\n",
        "            else:\n",
        "                self.total_energy_ai += self.temperature_ai - self.optimal_temperature[1]\n",
        "                self.temperature_ai = self.optimal_temperature[1]\n",
        "        \n",
        "        # UPDATING THE SCORES\n",
        "        \n",
        "        # Updating the Total Energy spent by the AI\n",
        "        self.total_energy_ai += energy_ai\n",
        "        # Updating the Total Energy spent by the server's cooling system when there is no AI\n",
        "        self.total_energy_noai += energy_noai\n",
        "        \n",
        "        # SCALING THE NEXT STATE\n",
        "        \n",
        "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / (self.max_temperature - self.min_temperature)\n",
        "        scaled_number_users = (self.current_number_users - self.min_number_users) / (self.max_number_users - self.min_number_users)\n",
        "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / (self.max_rate_data - self.min_rate_data)\n",
        "        next_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])\n",
        "        \n",
        "        # RETURNING THE NEXT STATE, THE REWARD, AND GAME OVER\n",
        "        \n",
        "        return next_state, self.reward, self.game_over\n",
        "\n",
        "    # MAKING A METHOD THAT RESETS THE ENVIRONMENT\n",
        "    \n",
        "    def reset(self, new_month):\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[new_month]\n",
        "        self.initial_month = new_month\n",
        "        self.current_number_users = self.initial_number_users\n",
        "        self.current_rate_data = self.initial_rate_data\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        self.temperature_ai = self.intrinsic_temperature\n",
        "        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0\n",
        "        self.total_energy_ai = 0.0\n",
        "        self.total_energy_noai = 0.0\n",
        "        self.reward = 0.0\n",
        "        self.game_over = 0\n",
        "        self.train = 1\n",
        "\n",
        "    # MAKING A METHOD THAT GIVES US AT ANY TIME THE CURRENT STATE, THE LAST REWARD AND WHETHER THE GAME IS OVER\n",
        "    \n",
        "    def observe(self):\n",
        "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / (self.max_temperature - self.min_temperature)\n",
        "        scaled_number_users = (self.current_number_users - self.min_number_users) / (self.max_number_users - self.min_number_users)\n",
        "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / (self.max_rate_data - self.min_rate_data)\n",
        "        current_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])\n",
        "        return current_state, self.reward, self.game_over\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whvh-7IiskEC",
        "colab_type": "text"
      },
      "source": [
        "# **Step2: Building the Brain with Dropout**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riqlYtajsifW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AI for Business - Minimize cost with Deep Q-Learning\n",
        "# Building the Brain with Dropout\n",
        "\n",
        "# Importing the libraries\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# BUILDING THE BRAIN\n",
        "\n",
        "class Brain(object):\n",
        "    \n",
        "    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD\n",
        "    \n",
        "    def __init__(self, learning_rate = 0.001, number_actions = 5):\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # BUILDIND THE INPUT LAYER COMPOSED OF THE INPUT STATE\n",
        "        states = Input(shape = (3,))\n",
        "        \n",
        "        # BUILDING THE FIRST FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\n",
        "        x = Dense(units = 64, activation = 'sigmoid')(states)\n",
        "        x = Dropout(rate = 0.1)(x)\n",
        "        \n",
        "        # BUILDING THE SECOND FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\n",
        "        y = Dense(units = 32, activation = 'sigmoid')(x)\n",
        "        y = Dropout(rate = 0.1)(y)\n",
        "        \n",
        "        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER\n",
        "        q_values = Dense(units = number_actions, activation = 'softmax')(y)\n",
        "        \n",
        "        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT\n",
        "        self.model = Model(inputs = states, outputs = q_values)\n",
        "        \n",
        "        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER\n",
        "        self.model.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVRnMPgUtBB5",
        "colab_type": "text"
      },
      "source": [
        "# **step 3: Implementing Deep Q-Learning with Experince Replay**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs5ttwHMtEpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AI for Business - Minimize cost with Deep Q-Learning\n",
        "# Implementing Deep Q-Learning with Experience Replay\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "\n",
        "# IMPLEMENTING DEEP Q-LEARNING WITH EXPERIENCE REPLAY\n",
        "\n",
        "class DQN(object):\n",
        "    \n",
        "    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE DQN\n",
        "    def __init__(self, max_memory = 100, discount = 0.9):\n",
        "        self.memory = list()\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "\n",
        "    # MAKING A METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY\n",
        "    def remember(self, transition, game_over):\n",
        "        self.memory.append([transition, game_over])\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    # MAKING A METHOD THAT BUILDS TWO BATCHES OF INPUTS AND TARGETS BY EXTRACTING TRANSITIONS FROM THE MEMORY\n",
        "    def get_batch(self, model, batch_size = 10):\n",
        "        len_memory = len(self.memory)\n",
        "        num_inputs = self.memory[0][0][0].shape[1]\n",
        "        num_outputs = model.output_shape[-1]\n",
        "        inputs = np.zeros((min(len_memory, batch_size), num_inputs))\n",
        "        targets = np.zeros((min(len_memory, batch_size), num_outputs))\n",
        "        for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))):\n",
        "            current_state, action, reward, next_state = self.memory[idx][0]\n",
        "            game_over = self.memory[idx][1]\n",
        "            inputs[i] = current_state\n",
        "            targets[i] = model.predict(current_state)[0]\n",
        "            Q_sa = np.max(model.predict(next_state)[0])\n",
        "            if game_over:\n",
        "                targets[i, action] = reward\n",
        "            else:\n",
        "                targets[i, action] = reward + self.discount * Q_sa\n",
        "        return inputs, targets\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsPavzvptNgY",
        "colab_type": "text"
      },
      "source": [
        "# **Step 4: Traning the AI with Early Stopping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCzsnF-ZtN8l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "11857dcf-f384-4250-dd21-2890e48b8501"
      },
      "source": [
        "# AI for Business - Minimize cost with Deep Q-Learning\n",
        "# Training the AI with Early Stopping\n",
        "\n",
        "# Importing the libraries and the other python files\n",
        "import os\n",
        "import numpy as np\n",
        "import random as rn\n",
        "\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "rn.seed(12345)\n",
        "\n",
        "# SETTING THE PARAMETERS\n",
        "epsilon = .3\n",
        "number_actions = 5\n",
        "direction_boundary = (number_actions - 1) / 2\n",
        "number_epochs = 100\n",
        "max_memory = 3000\n",
        "batch_size = 512\n",
        "temperature_step = 1.5\n",
        "\n",
        "# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
        "env = Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)\n",
        "\n",
        "# BUILDING THE BRAIN BY SIMPLY CREATING AN OBJECT OF THE BRAIN CLASS\n",
        "brain = Brain(learning_rate = 0.00001, number_actions = number_actions)\n",
        "\n",
        "# BUILDING THE DQN MODEL BY SIMPLY CREATING AN OBJECT OF THE DQN CLASS\n",
        "dqn = DQN(max_memory = max_memory, discount = 0.9)\n",
        "\n",
        "# CHOOSING THE MODE\n",
        "train = True\n",
        "\n",
        "# TRAINING THE AI\n",
        "env.train = train\n",
        "model = brain.model\n",
        "early_stopping = True\n",
        "patience = 10\n",
        "best_total_reward = -np.inf\n",
        "patience_count = 0\n",
        "if (env.train):\n",
        "    # STARTING THE LOOP OVER ALL THE EPOCHS (1 Epoch = 5 Months)\n",
        "    for epoch in range(1, number_epochs):\n",
        "        # INITIALIAZING ALL THE VARIABLES OF BOTH THE ENVIRONMENT AND THE TRAINING LOOP\n",
        "        total_reward = 0\n",
        "        loss = 0.\n",
        "        new_month = np.random.randint(0, 12)\n",
        "        env.reset(new_month = new_month)\n",
        "        game_over = False\n",
        "        current_state, _, _ = env.observe()\n",
        "        timestep = 0\n",
        "        # STARTING THE LOOP OVER ALL THE TIMESTEPS (1 Timestep = 1 Minute) IN ONE EPOCH\n",
        "        while ((not game_over) and timestep <= 5 * 30 * 24 * 60):\n",
        "            # PLAYING THE NEXT ACTION BY EXPLORATION\n",
        "            if np.random.rand() <= epsilon:\n",
        "                action = np.random.randint(0, number_actions)\n",
        "                if (action - direction_boundary < 0):\n",
        "                    direction = -1\n",
        "                else:\n",
        "                    direction = 1\n",
        "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "            # PLAYING THE NEXT ACTION BY INFERENCE\n",
        "            else:\n",
        "                q_values = model.predict(current_state)\n",
        "                action = np.argmax(q_values[0])\n",
        "                if (action - direction_boundary < 0):\n",
        "                    direction = -1\n",
        "                else:\n",
        "                    direction = 1\n",
        "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "            # UPDATING THE ENVIRONMENT AND REACHING THE NEXT STATE\n",
        "            next_state, reward, game_over = env.update_env(direction, energy_ai, ( new_month + int(timestep/(30*24*60)) ) % 12)\n",
        "            total_reward += reward\n",
        "            # STORING THIS NEW TRANSITION INTO THE MEMORY\n",
        "            dqn.remember([current_state, action, reward, next_state], game_over)\n",
        "            # GATHERING IN TWO SEPARATE BATCHES THE INPUTS AND THE TARGETS\n",
        "            inputs, targets = dqn.get_batch(model, batch_size = batch_size)\n",
        "            # COMPUTING THE LOSS OVER THE TWO WHOLE BATCHES OF INPUTS AND TARGETS\n",
        "            loss += model.train_on_batch(inputs, targets)\n",
        "            timestep += 1\n",
        "            current_state = next_state\n",
        "        # PRINTING THE TRAINING RESULTS FOR EACH EPOCH\n",
        "        print(\"\\n\")\n",
        "        print(\"Epoch: {:03d}/{:03d}\".format(epoch, number_epochs))\n",
        "        print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
        "        print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\n",
        "        # EARLY STOPPING\n",
        "        if (early_stopping):\n",
        "            if (total_reward <= best_total_reward):\n",
        "                patience_count += 1\n",
        "            elif (total_reward > best_total_reward):\n",
        "                best_total_reward = total_reward\n",
        "                patience_count = 0\n",
        "            if (patience_count >= patience):\n",
        "                print(\"Early Stopping\")\n",
        "                break\n",
        "        # SAVING THE MODEL\n",
        "        model.save(\"model.h5\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch: 001/100\n",
            "Total Energy spent with an AI: 146\n",
            "Total Energy spent with no AI: 223\n",
            "\n",
            "\n",
            "Epoch: 002/100\n",
            "Total Energy spent with an AI: 130\n",
            "Total Energy spent with no AI: 290\n",
            "\n",
            "\n",
            "Epoch: 003/100\n",
            "Total Energy spent with an AI: 123\n",
            "Total Energy spent with no AI: 273\n",
            "\n",
            "\n",
            "Epoch: 004/100\n",
            "Total Energy spent with an AI: 96\n",
            "Total Energy spent with no AI: 170\n",
            "\n",
            "\n",
            "Epoch: 005/100\n",
            "Total Energy spent with an AI: 6\n",
            "Total Energy spent with no AI: 14\n",
            "\n",
            "\n",
            "Epoch: 006/100\n",
            "Total Energy spent with an AI: 216\n",
            "Total Energy spent with no AI: 490\n",
            "\n",
            "\n",
            "Epoch: 007/100\n",
            "Total Energy spent with an AI: 16\n",
            "Total Energy spent with no AI: 33\n",
            "\n",
            "\n",
            "Epoch: 008/100\n",
            "Total Energy spent with an AI: 18\n",
            "Total Energy spent with no AI: 43\n",
            "\n",
            "\n",
            "Epoch: 009/100\n",
            "Total Energy spent with an AI: 165\n",
            "Total Energy spent with no AI: 319\n",
            "\n",
            "\n",
            "Epoch: 010/100\n",
            "Total Energy spent with an AI: 24\n",
            "Total Energy spent with no AI: 54\n",
            "\n",
            "\n",
            "Epoch: 011/100\n",
            "Total Energy spent with an AI: 120\n",
            "Total Energy spent with no AI: 231\n",
            "\n",
            "\n",
            "Epoch: 012/100\n",
            "Total Energy spent with an AI: 90\n",
            "Total Energy spent with no AI: 196\n",
            "\n",
            "\n",
            "Epoch: 013/100\n",
            "Total Energy spent with an AI: 8\n",
            "Total Energy spent with no AI: 15\n",
            "\n",
            "\n",
            "Epoch: 014/100\n",
            "Total Energy spent with an AI: 2\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 015/100\n",
            "Total Energy spent with an AI: 120\n",
            "Total Energy spent with no AI: 223\n",
            "\n",
            "\n",
            "Epoch: 016/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 0\n",
            "Early Stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIMxP5dqtqwU",
        "colab_type": "text"
      },
      "source": [
        "# **Step 5:Testing the AI**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjkx2H7utrLn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "691fc04e-9a21-4b31-bbe7-d9b2f2dbe6ab"
      },
      "source": [
        "# AI for Business - Minimize cost with Deep Q-Learning\n",
        "# Testing the AI\n",
        "\n",
        "# Installing Keras\n",
        "# conda install -c conda-forge keras\n",
        "\n",
        "# Importing the libraries and the other python files\n",
        "import os\n",
        "import numpy as np\n",
        "import random as rn\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "rn.seed(12345)\n",
        "\n",
        "# SETTING THE PARAMETERS\n",
        "number_actions = 5\n",
        "direction_boundary = (number_actions - 1) / 2\n",
        "temperature_step = 1.5\n",
        "\n",
        "# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
        "env = Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)\n",
        "\n",
        "# LOADING A PRE-TRAINED BRAIN\n",
        "model = load_model(\"model.h5\")\n",
        "\n",
        "# CHOOSING THE MODE\n",
        "train = False\n",
        "\n",
        "# RUNNING A 1 YEAR SIMULATION IN INFERENCE MODE\n",
        "env.train = train\n",
        "current_state, _, _ = env.observe()\n",
        "for timestep in range(0, 12 * 30 * 24 * 60):\n",
        "    q_values = model.predict(current_state)\n",
        "    action = np.argmax(q_values[0])\n",
        "    if (action - direction_boundary < 0):\n",
        "        direction = -1\n",
        "    else:\n",
        "        direction = 1\n",
        "    energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "    next_state, reward, game_over = env.update_env(direction, energy_ai, int(timestep / (30 * 24 * 60)))\n",
        "    current_state = next_state\n",
        "\n",
        "# PRINTING THE TRAINING RESULTS FOR EACH EPOCH\n",
        "print(\"\\n\")\n",
        "print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
        "print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\n",
        "print(\"ENERGY SAVED: {:.0f} %\".format((env.total_energy_noai - env.total_energy_ai) / env.total_energy_noai * 100))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Energy spent with an AI: 1575633\n",
            "Total Energy spent with no AI: 1978293\n",
            "ENERGY SAVED: 20 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}